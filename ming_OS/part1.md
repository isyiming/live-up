# mingOS
### 系统的虚拟内存实现
### 配合《深入理解计算机系统》食用
---------------

我们首先来愉快的看下系统的虚拟内存是如何实现的吧～

还记得cfenollosa在他的os-tutorial最后一节添加的那个kmalloc()函数吗？
```c
/* This should be computed at link time, but a hardcoded
 * value is fine for now. Remember that our kernel starts
 * at 0x1000 as defined on the Makefile */
uint32_t free_mem_addr = 0x10000;
/* Implementation is just a pointer to some free memory which
 * keeps growing */
uint32_t kmalloc(size_t size, int align, uint32_t *phys_addr) {
    /* Pages are aligned to 4K, or 0x1000 * /
    if (align == 1 && (free_mem_addr & 0x00000FFF)) {
        free_mem_addr &= 0xFFFFF000;
        free_mem_addr += 0x1000;
    }
    /* Save also the physical address * /
    if (phys_addr){
        *phys_addr = free_mem_addr;
    }

    uint32_t ret = free_mem_addr;
    free_mem_addr += size; /* Remember to increment the pointer * /
    return ret;
}
```
这个函数可以为内核返回未使用的内存空间地址。但是当时他的实现有缺陷。free_mem_addr时刻记录着未分配的地址的起点。这是内核启动后就开始调用的函数，此时内核代码刚刚加载到主存中。除了内核代码，还有中断向量表IDT和全局描述符表GDT这些内核在启动时加载到主存中的数据结构。还记不记得GDT要在内核启动之前就要加载好，所以它开始只能放在1M以下的地址空间中。总之，我们知道内核代码加载，并且初始化一些数据结构后，较高地址空间都是未使用的。那么我们设定一个起始地址free_mem_addr，由free_mem_addr～4GB为内核分配内存就好了。

free_mem_addr就是时刻记录着未经使用过的内存段的起始地址。这个其实地址最开始应该是内核占用内存的结束位置，应该在链接的时候记录下来的。但是cfenollosa直接指定了一个地址：0x10000。
我们看下makefile文件中的链接指令：
```makefile
i386-elf-ld -o $@ -Ttext 0x1000 $^ --oformat binary
```
-Ttext 0x1000 指定了代码段的起始地址是0x1000。而free_mem_addr的起始地址是0x10000，看来cfenollosa默认了自己的代码段长度没超过0xF000。我就感觉这个地址很奇怪，既然你不知道内核占用内存的结束位置，那你就设定一个安全一点的位置呀，怎么也应该是1M以后的地址。因为实模式只能寻址到1M。

好了，不管这个了。我们说下真正可靠的方法是如何做的吧。我们要在链接的时候记录下内核加载到内存的结束位置。
```
/* Link.ld -- Linker script for the kernel - ensure everything goes in the */
/*            Correct place.  */
/*            Original file taken from Bran's Kernel Development */
/*            tutorials: http://www.osdever.net/bkerndev/index.php. */
/* 这是 JamesM 的教程中的链接文件，ming在此添加了记录内核代码段起始位置和内核占用内存结束位置的两个变量：kern_start 和kern_end
ENTRY(start)
SECTIONS
{
  . = 0x1000;
  PROVIDE( kern_start = . );
  .text  :
  {
    code = .; _code = .; __code = .;
    *(.text)
    . = ALIGN(4096);
  }

  .data :
  {
     data = .; _data = .; __data = .;
     *(.data)
     *(.rodata)
     . = ALIGN(4096);
  }

  .bss :
  {
    bss = .; _bss = .; __bss = .;
    *(.bss)
    . = ALIGN(4096);
  }

  PROVIDE( kern_end = . );

  end = .; _end = .; __end = .;
}
```
将这个link.ld文件放在makefile的同级目录下，我们再修改下Makefile文件中的链接指令
```makefile
# '--oformat binary' deletes all symbols as a collateral, so we don't need
# to 'strip' them manually on this case
kernel.bin: boot/kernel_entry.o ${OBJ}
	# i386-elf-ld -o $@ -Ttext 0x1000 $^ --oformat binary
	i386-elf-ld -Tlink.ld -melf_i386 -o $@ $^ --oformat binary

# Used for debugging purposes
kernel.elf: boot/kernel_entry.o ${OBJ}
	# i386-elf-ld -o $@ -Ttext 0x1000 $^
	i386-elf-ld -Tlink.ld -melf_i386 -o $@ $^
```
原来cfenollosa的代码（就是被我注释掉的部分），-Ttext 0x1000 的意思是指定了代码段的起始地址是0x1000。我们不再直接在链接指令中指定代码段的起始位置了，而是在链接脚本中指定。并在link.ld中用kern_start 和kern_end两个变量记录下代码段的起始地址和内核加载到内存的结束位置。这两个变量在.c文件中extern下就可以使用了。
我们在men.h中添加如下代码：
```c
extern uint8_t kern_start[];//链接的时候指定的内核代码段起始位置，在link.ld中
extern uint8_t kern_end[];
```
由于kernel.c包含了men.h 就可以直接使用这两个变量了。那就在内核启动的时候，打印下这两个变量吧
```c
void kernel_main() {
    isr_install();
    irq_install();

    asm("int $2");
    asm("int $3");

    kprint("Type something, it will go through the kernel\n"
        "Type END to halt the CPU or PAGE to request a kmalloc()\n> ");

    char kern_start_str[16] = "";
    char kern_end_str[16] = "";
    hex_to_ascii(kern_start, kern_start_str);
    hex_to_ascii(kern_end, kern_end_str);
    kprint("kernel in memory start: ");      kprint(kern_start_str);      kprint("\n");
    kprint("kernel in memory end  : ");      kprint(kern_end_str);      kprint("\n");
}
```
记得更改程序后，再make之前要先make clean下，清除之前的目标文件，因为我们这次链接部分有了改动。不然就找不到kern_end和kern_end_str了。最后启动系统后的结果，会首先输出下：
```c
> kernel in memory start: 0x1000
kernel in memory end  : 0x4000
```

我们在链接的时候除了text段，还申请了data和bss段，实际上除了GDT和IDT，现在我还不确定内核还加载了那些数据结构到主存中。我印象中应该是没有。而GDT和IDT加载到了什么位置，我之前看cfenollosa的教程的时候没有注意，但是今天也懒得回去看啦～









1. 创建了kmalloc.c和kmalloc.h 其中包含




### 内存管理的解决办法-虚拟内存

1. 内存分页：
  将物理内存空间分割为4kb大小的页，32位机器可以寻址4GB内存空间。那么就可以分出来1MB页。
2. 主存作为磁盘的缓存：
  我们将磁盘中某一段区域也分割称4kb大小的页，将主存中不活跃的页暂存在磁盘中，主存中保存这活跃的页。这样可以最大限度的利用有限的主存，让更多进程时刻调用的页保持在主存中。

  这里要注意下，由于磁盘的构造。他随机读取一个字节的代价非常高昂。而且往往在使用一段信息的时候，它的上下文也是有可能在下一时刻被使用。数据库的B-tree就是这样设计的嘛。所以对磁盘一次仅仅读取一个字节不合理。所以我们一般将页设计成4kb～2MB。

3. 页表和PTE：
  我们定义出一个虚拟页出来，这里虚拟页就是一个概念，而不是实体。虚拟页到物理页的映射用页表条目（page table entry，PTE）来保存。也就是说，根据PTE就可以知道某一个虚拟页的物理地址是多少了。这一页可能保存在主存中，也可能保存在磁盘中。

  页表条目PTE由一个有效位和一个n位地址段组成，有效位为1，表示该页已经加载到主存中，为0但是地址段为null，那就是这一页还没有分配。为0但是地址位有一个指向磁盘的有效地址。

  所有的虚拟内存对应的PTE保存在一个数组里，就是页表，页表保存在主存中。

4. 多级页表
  如果一个PTE是4字节的话，那么1MB个PTE总共就是4MB。如果你之前已经知道了进程的概念，就不难理解这4MB还是太大了。因为每个进程都要有一个页表。现在打开你的电脑看下活动监视器或者任务管理器，我想同时打开的应用软件肯定20多个肯定有了吧，再看下每个应用下有多少个进程呢？这么算，单单是页表就要在主存中占用几百MB的空间了。而主存一共才1024*4MB呀。如果是64位系统，由于可寻址内存地址更大了，那么页表也会变得更大。

  解决办法就是构建多级页表。

  一级页表中的每个PTE负责映射虚拟地址空间的一个4MB的片，每一片中有1024个连续的页。二级页表中的每个PTE负责映射虚拟地址空间的一个4kB的页。具体到实际的32位的地址寻址上，一级页表条目保存着高10位，对应着1024个4MB空间；二级页表保存着中间10位，对应着1024个4KB空间；剩下的12位正好代表4kb的地址偏移量。

  如果一级页表中的某一片是空的，那么二级页表就不会存在。只有一级页表需要总是存在主存中，频繁使用的二级页表缓存在主存中。

1.虚拟内存
  为了更高效的管理多进程系统的内存。
  虚拟内存是硬件异常，硬件地址翻译，主存，磁盘文件，和内核软件的完美结合
  它为每一个进程提供一个大的，一致的私有的空间。
  虚拟内存提供了三种能力：
    1）将主存看作是磁盘地址空间的一个高速缓存，在主存中只保留活动区域
    2）它为每个进程提供了一致的地址空间，简化内存管理
    3）保护每个进程之间的地址空间独立，互相不破坏


3.虚拟内存作为缓存的工具
  我们认为设定了一个概念，虚拟内存。这个虚拟内存被组织为一个存在磁盘中的数组，数组的每一个字节都有一个唯一的虚拟地址，作为数组的索引。
  磁盘上的这个数组，活跃部分被缓存到主存中。
  这个数组被分割成块，主存和磁盘之间的传输最小单位就是块。实际上就是虚拟内存的页。
  虚拟内存系统将虚拟内存分割为虚拟页，每一页大小是P字节。对应物理内存也得分成物理页，这是方便我们载入页到实际的主存中。
  那么虚拟内存的全部页可以分为三种情况：
    1）未分配的
    2）缓存的
    3）未缓存的

6.缺页
  缺页异常，当CPU引用某一页的VP3，但是在页表PTE3中发现它的有效位是0。说明这一页没有加载到DRAM中，那就需要从磁盘中加载这一页。
  于是触发系统缺页异常，异常处理程序会在主存中选择一个牺牲页VP4。如果该牺牲页已经被修改了(和磁盘中的相比)，那就将它复制回磁盘。
  接下来内核会从磁盘中复制VP3到内存中的pp3中。这就完成了缺页的处理。
  页从磁盘和主存之间的传送叫做页面调度，一般都是用按需调度，只有在不明中的时候才调度。

7.程序的局部性保证了缺页异常不会经常发生，从而保证了虚拟内存的高效率。
  虚拟内存是用低速的磁盘作为主存的拓展，如果频繁的发生页面调度，那么肯定会拉低程序的运行效率。
  但是进程在执行过程中并不是频繁的跳来跳去的，因为cpu只能执行当前代码段，所以它访问的虚拟内存的页也是有限的。
  如果我我们不引入虚拟内存，那么多进程的内存管理实在是太麻烦了。程序执行效率肯定一定会更低，除非你有比虚拟内存更好的办法。

  程序运行时，短时间内总是趋向于在一个较小的活动页面集合上工作，这个集合叫做工作集，或者常驻集合。
  如果工作集超过了物理内存的大小，那程序将处于一种“抖动状态”，页面将频繁的调度。
  这时候就效率太低了，是不是程序设计的太不合理了，一次读取了太大的数据。

8.虚拟内存作为内存管理工具带来的好处
  1）简化了链接：因为在虚拟内存中，每个进程有自己独立的内存空间，这个内存空间的真实地址由系统和CPU的内存管理单元翻译。
  所以链接的时候不需要考虑程序在主存中的真实地址是多少，只要将每个程序按照相同的格式翻译好就可以了。
  比如，链接器对不同的程序，总是可以设置成代码段从0x400000开始，静态数据段跟在代码段之后，然后是堆，从高地址往下是栈。
  这样的一致性对于链接器来讲实在是太方便了，我们终于看到系统带来的巨大好处。
  它将内存地址这种具体的硬件抽象，对于上层的应用而言，不用针对设备具体设计程序。
  2）简化加载：Linux系统加载器加载可执行文件的时候，只需要为代码段和数据段分配虚拟页，把他们标记为无效的（未缓存的）就可以了。这时候他们还在磁盘上，只有系统调用该页的时候，产生了缺页异常，系统才会加载该页到主存。这样看来，还是系统帮我们解决了和硬件打交道的工作，我们编写的程序只需要满足系统指定的规则就可以了。
  将一组连续虚拟页映射到一个文件的任意位置，叫做内存映射。Linux提供一个mmap的系统调用，允许应用程序自己做内存映射。
  3）简化共享：如果多个进程都需要调用一些系统代码，系统调用。那么操作系统就可以将不同进程中的适当虚拟页映射到相同的物理页上（它们共同调用的那些系统代码段）。
  4）简化内存分配：

9.虚拟内存作为保护内存的工具
  通过PTE的上的限制，比如在PTE上加上额外的许可位来限制进程的访问。SUP表示只有内核才能访问，READ和WRITE位表示读写权限。
  如果一个进程违反了这个许可条件，就会触发一个一般保护故障，将控制传递给一个内核中的异常处理程序。shell一般将这中异常报告为“段错误”。
10.地址翻译
  1）处理器产生一个虚拟地址，将它传递给MMU。
    实际上对于处理器来将，它不知道这个地址是不是虚拟的，它只负责要一个地址的数据。但是这个地址是不是有效的那是虚拟内存机制的来处理的。
  2）MMU（内存管理单元）生成PTE地址，并向主存请求得到它。其实就是MMU查阅页表。
  3）主存将PTE返回给MMU。查阅页表成功了。
  4）MMU根据PTE构造物理地址，并从主存中读取这个地址里的数据。   这一步要是缺页了，就会触发异常。那cpu就执行中断程序了，那是另一个程序逻辑了，我们不用理会。反正最后，主存要载入这一页的数据。
  5）主存将数据传递给cpu。

11.利用TLB（翻译后备缓冲器）
  CPU每调用一次主存中的数据，MMU就要在主存中查阅一次PTE，PTE数据量很小，但是MMU访问主存这个时间相比CPU的时钟周期太慢了。所以通常在SRAM（L1）中有一个PTE的缓存区，因为SRAM是比主存还要高速的设备，所以减少了频繁的访问主存，而只为了一条PTE数据。



13.内存映射
  Linux系统将一个虚拟内存区域与一个磁盘上的对象关联
  虚拟内存区域可以映射到两种类型的文件中的一种：
    1）Linux文件系统中的普通文件，这种情况下只是将PTE更改了，但是磁盘上的文件没有真正复制到主存中，只有cpu访问该文件触发缺页异常后，才载入主存。
    2）匿名文件，映射到匿名文件时，这个就是由内核创建的了，这个文件中都是二进制0.然后如果可以的话，直接将内存中的该页写为二进制0.这样磁盘和主存之间没有数据传送
    无论那种情况，一旦一个虚拟页被创建了，它就在一个由内核维护的专门的交换文件（swap file）之间换来换去。任何时刻，交换空间都限制着当前进程能够分配的虚拟页面数目。

交换空间 交换空间swap space是啥？

14.内存映射和共享对象
  进程这一抽象可以为每个进程提供独立的私有虚拟内存空间，但是有时候有些数据是多个进程共用的。例如程序包含的库文件，系统调用。如果每个进程都在物理内存中保留它们的副本，那太占用内存了。
  一个对象如果可以被映射到虚拟内存中的一个区域，要么作为共享对象，要么作为私有对象。

  如果一个对象被映射为共享对象，那么共享该对象的进程都可以看到这个对象的改动。
  如果一个对象被映射为私有对象，那么该对象只对它所在的进程可见。
  每个对象都有唯一的文件名，所以内核可以快速判定进程1已经映射了这个对象。（文件系统这里还没看呢呀）

15.私有对象的写时复制 copy-on-write
  如果两个进程创建时有一个相同的私有对象，比如两个进程都调用了同一个库，但是都设置为私有的。
  那么在物理内存中只会有一个私有对象真实存在，只有当其中一个进程要对一个私有区域的某个页面时，就会触发一个保护故障。如果保护故障程序发现保护异常是由于进程对私有写时复制区域进行修改引起的，那就将该对象在物理内存中创建一个新的副本。这样做还是为了充分利用稀有的内存资源。

16. 再看fork函数
  fork函数调用一次，返回两次。返回给父进程子进程的PID。返回给子进程0。如果出现错误，就返回一个负值。
  fork出的子进程会完全复制父进程当前的状态，所以它被创建后会和父进程同步继续执行下去。

  当fork函数被当前进程调用后，内核会为新的进程创建各种数据结构，并分配给它一个唯一的PID（进程标识符，process ID）。它创建了当前进程的mm_struct,区域结构和页表的原样副本。它将两个进程的每个页面都标记为只读，并将两个进程中的每个私有区域都标记为写时复制。
  当fork函数在新的子进程返回时，新进程现在的虚拟内存刚好和调用fork时存在的虚拟内存相同。当两个进程中的任意一个对私有区域进行写操作时，写时复制机制就会在物理内存中创建新页面。

17. 再看execve函数
  execve函数在当前进程中加载并运行包含在可执行目标文件a.out，用a.out有效地替代了当前程序。加载并运行a.out需要如下步骤：
    删除已经存在的用户区域。删除当前进程虚拟地址的用户部分中已经存在的区域结构。
    映射私有区域。为新程序的代码，数据，bss和栈区域创建新的区域结构。所有的这些新的区域都是私有的，写时复制的。
    映射共享区域。如果a.out与共享对象链接，那么这些对象就会动态的链接到这个程序，然后在映射到用户虚拟地址空间中的共享区域。
    设置程序计数器。execve做的最后一件事就是设置当前进程上下文的程序计数器，这样子进程和父进程就同步执行了。

18. 使用mmap函数的用户级内存映射
  Linux进程可以使用mmap函数来创建新的虚拟内存区域，并将对象映射到这些区域中。

15. 共享对象

15.Linux的虚拟内存系统
  系统为每个进程都维护一个单独的虚拟内存空间，从0x4000 0000开始是：代码段.text，静态存储区和全局变量.data，未初始化的数据.bss, heap堆（进程调用malloc分配内存的时候，新的内存就分配到堆上），共享内存区（默认大小是32M），stack栈（用户创建的局部变量），内核代码和数据，物理内存，与进程相关的数据结构，如页表等。


16. Linux的缺页异常处理





1.Linux下一个进程可以开多少线程
  取决于设定的线程最小栈空间大小
  32位linux系统最大内存地址4G，0-3GB的给用户进程(User Space)使用，3-4GB给内核使用
　stack size (kbytes,-s)10240表示线程堆栈大小,3G/10M=最大线程数，











1
